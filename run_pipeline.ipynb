{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pystackreg import StackReg\n",
    "\n",
    "import utils as utl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of channels and z slices\n",
    "n_ch, n_z = 2, 15\n",
    "# frequencies used for imaging, ball velocities, and behavior\n",
    "f_ca, f_ball, f_beh = 2, 50, 200\n",
    "\n",
    "# transformation used for registration https://pystackreg.readthedocs.io/en/latest/readme.html#usage\n",
    "reg = StackReg.SCALED_ROTATION \n",
    "\n",
    "# path to folder\n",
    "parent_dir = Path(r'\\\\mpfi.org\\public\\sb-lab\\Nino_2P_for_Salil\\for_Nico\\stop1_imaging\\stop1-GCaMP6f-tdTomato_VNC\\fed')\n",
    "\n",
    "# selection rule for tif files\n",
    "p_tifs = [ p for p in parent_dir.glob('**/trials_to_register/*/trial*_00???.tif') ]\n",
    "\n",
    "# force overwriting files\n",
    "overwrite = False\n",
    "\n",
    "# collect all data in this file\n",
    "p_all = Path('./all_data.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_tif in p_tifs:\n",
    "    print()\n",
    "\n",
    "    if utl.fname('ch1.tif').is_file() and not overwrite:\n",
    "        print(f'INFO output file exists, skipping registration for {p_tif.parent}')\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        print(f'INFO now registering {p_tif}')\n",
    "\n",
    "    # load and split\n",
    "    stack = utl.load_tiff(p_tif)\n",
    "    ch1, ch2 = utl.split_channels(stack, n_z=15, n_ch=2)\n",
    "    ch1 = utl.maxproj_z(ch1)\n",
    "    ch2 = utl.maxproj_z(ch2)\n",
    "\n",
    "    # register\n",
    "    tmats = utl.get_tmats(ch2, reg)\n",
    "    ch1_a = utl.align(ch1, tmats, reg)\n",
    "    ch2_a = utl.align(ch2, tmats, reg)\n",
    "\n",
    "    # mean image\n",
    "    ch1_am = np.mean(ch1_a, axis=0)\n",
    "    ch2_am = np.mean(ch2_a, axis=0)\n",
    "\n",
    "    # save to disk\n",
    "    utl.write_tif(utl.fname('ch1.tif'), ch1_a.astype('int16'))\n",
    "    utl.write_tif(utl.fname('ch2.tif'), ch2_a.astype('int16'))\n",
    "\n",
    "    utl.write_tif(utl.fname('ch1reg.tif'), ch1_a.astype('int16'))\n",
    "    utl.write_tif(utl.fname('ch2reg.tif'), ch2_a.astype('int16'))\n",
    "\n",
    "    utl.save_img(utl.fname('ch1mean.bmp'), ch1_am)\n",
    "    utl.save_img(utl.fname('ch2mean.bmp'), ch2_am)\n",
    "\n",
    "    utl.save_dual_movie(utl.fname('ch1ch2.mp4'), ch1, ch2)\n",
    "    utl.save_dual_movie(utl.fname('ch1reg.mp4'), ch1, ch1_a)\n",
    "    utl.save_dual_movie(utl.fname('ch2reg.mp4'), ch2, ch2_a)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: ROI extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_tif in p_tifs:\n",
    "    print()\n",
    "    \n",
    "    # check if ROI traces have already been extracted\n",
    "    p_roi = utl.fname('roi_traces.npy')\n",
    "    if p_roi.is_file() and not overwrite:\n",
    "        print(f'INFO output files exists, skipping ROI extraction for {p_tif.parent}')\n",
    "        continue\n",
    "\n",
    "    # load Roi.zip\n",
    "    p_zip = utl.get_roi_zip_file(p_tif)\n",
    "    if not p_zip:\n",
    "        print(f'WARNING Skipping {p_tif.parent}')\n",
    "        continue\n",
    "    else:\n",
    "        print(f'INFO loading ROIs from {p_zip}')\n",
    "\n",
    "    # load aligned ch1\n",
    "    stack = utl.load_tiff(utl.fname('ch1reg.tif'))\n",
    "    img = np.mean(stack, axis=0)\n",
    "\n",
    "    # load ROIs\n",
    "    rois = utl.read_imagej_rois(p_zip, img)\n",
    "    img_rois = utl.draw_rois(img, rois)\n",
    "    utl.save_img(utl.fname('ch1mean_rois.bmp'), img_rois)\n",
    "\n",
    "    # extract traces\n",
    "    ca = utl.get_mean_trace(rois, stack, subtract_background=True, sigma=0)\n",
    "    np.save(p_roi, ca)    \n",
    "    print(f'INFO saving ROI traces to {p_roi}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Combine imaging and behavior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_tif in p_tifs:\n",
    "    print()\n",
    "    \n",
    "    # check if already been processed\n",
    "    p_df = utl.fname('data.parquet')\n",
    "    if p_df.is_file() and not overwrite:\n",
    "        print(f'INFO output files exists, skipping data merging for {p_tif.parent}')\n",
    "        continue\n",
    "\n",
    "    # load ROI traces\n",
    "    p_roi = utl.fname('roi_traces.npy')\n",
    "    if not p_roi.is_file():\n",
    "        print(f'WARNING file with ROI traces not found, skipping {p_tif.parent}')\n",
    "    else:\n",
    "        ca = np.load(p_roi)\n",
    "\n",
    "    # load behavior data and ball velocities\n",
    "    p_mats = utl.get_matlab_files(p_tif)\n",
    "    if not p_mats:\n",
    "        print(f'WARNING skipping {p_tif.parent}')\n",
    "    else:\n",
    "        p_ball, p_beh =  p_mats\n",
    "        \n",
    "    ball = utl.load_ball(p_ball)\n",
    "    beh = utl.load_behavior(p_beh)\n",
    "\n",
    "    # match sample rates\n",
    "    df = utl.upsample_to_behavior(ca, beh, ball, f_ca, f_ball, f_beh)\n",
    "    # zscore ROIs\n",
    "    df = utl.zscore_cols(df, col_start='roi_')\n",
    "    # convolute ball velocities and behavior with Ca kernel\n",
    "    df = utl.convolute_ca_kernel(df, f=f_beh)\n",
    "    # zscore ball velocities\n",
    "    df = utl.zscore_cols(df, col_start='conv_ball_')\n",
    "\n",
    "    # add additional data based on file and folder names\n",
    "    pt = p_tif.parts\n",
    "    cond, fly, trial = pt[-5], pt[-4], pt[-2]\n",
    "    df.loc[:, 'cond'] = cond # e.g. fed/starved\n",
    "    df.loc[:, 'fly'] = fly # fly number\n",
    "    df.loc[:, 'trial'] = trial # trial number\n",
    "    print(f'INFO parsing folder names: fly {fly} | trial {trial} | condition {cond}')\n",
    "\n",
    "    # plots for quality control\n",
    "    utl.plot_data(df, f_beh, path=utl.fname('data.png'))\n",
    "    # pearson r heatmap\n",
    "    utl.plot_corr_heatmap(df, beh='behi', path=utl.fname('heatmap.png'))\n",
    "    # ccf\n",
    "    utl.plot_ccf(df, f=f_beh, pool_fly=True, path=utl.fname('ccf.png'))\n",
    "\n",
    "    # save to disk\n",
    "    print(f'INFO writing merged data to {p_df}')\n",
    "    df.to_parquet(p_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: merge all trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all trials and flies\n",
    "\n",
    "# list of all *_data.parquet files\n",
    "p_pars = [ utl.fname(p, 'data.parquet') for p in p_tifs ]\n",
    "\n",
    "l = []\n",
    "for p_par in p_pars:\n",
    "    print()\n",
    "    if not p_par.is_file():\n",
    "        print(f'WARNING skipping {p_par.parent}')\n",
    "        continue\n",
    "    else:\n",
    "        print(f'INFO loading file {p_par}')\n",
    "        df = pd.read_parquet(p_par)\n",
    "        l.append(df)\n",
    "\n",
    "# combine dataframes and save\n",
    "df = pd.concat(l, ignore_index=True)\n",
    "df.to_parquet(p_all)\n",
    "print(f'INFO writing all data to {p_all}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from disk\n",
    "df = pd.read_parquet(p_all)\n",
    "df = df.fillna(0)\n",
    "print('INFO dataframe contains')\n",
    "for f, d in df.groupby('fly'):\n",
    "    print(f'     {f}', end=': ')\n",
    "    for t, _ in d.groupby('trial'):\n",
    "        print(f'{t}', end=' ')\n",
    "    print()\n",
    "\n",
    "# plot averages\n",
    "utl.plot_corr_heatmap(df, beh='behi', path=p_all.parent / 'heatmap.png')\n",
    "utl.plot_ccf(df, f=f_beh, pool_fly=True,  path=p_all.parent / 'ccf.png')\n",
    "utl.plot_ccf(df, f=f_beh, pool_fly=False, path=p_all.parent / 'ccf_indv.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imaging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
