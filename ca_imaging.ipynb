{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pystackreg import StackReg\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import pickle\n",
    "\n",
    "import utils as utl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of channels and z slices\n",
    "n_ch, n_z = 2, 15\n",
    "\n",
    "# smoothing in xy before registration (sigma 2D Gaussian)\n",
    "xy_smth = 4\n",
    "\n",
    "# settings for background subtraction: percentile and window size (winsize * f_ca = window in seconds)\n",
    "perc, winsize = 10, 50\n",
    "\n",
    "# frequencies used for imaging, ball velocities, and behavior\n",
    "f_ca, f_ball, f_beh = 2, 50, 200\n",
    "\n",
    "# transformation used for registration https://pystackreg.readthedocs.io/en/latest/readme.html#usage\n",
    "reg = StackReg.SCALED_ROTATION \n",
    "\n",
    "# path to folder\n",
    "parent_dir = Path(r'\\\\mpfi.org\\public\\sb-lab\\Nino_2P_for_Salil\\for_Nico\\stop1_imaging\\stop1-GCaMP6f-tdTomato_VNC\\fed')\n",
    "\n",
    "# selection rule for tif files\n",
    "p_tifs = [ *parent_dir.glob('**/trials_to_register/*/trial*_00???.tif') ]\n",
    "\n",
    "# force overwriting files\n",
    "overwrite = True\n",
    "\n",
    "# folder to save output\n",
    "p_out = parent_dir / 'smth4_perc10_winsize50'\n",
    "p_out.mkdir(exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# general processing pipeline\n",
    "## Step 1: Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_tif in p_tifs:\n",
    "    print()\n",
    "\n",
    "    # define plot folder\n",
    "    p_plot = utl.fname(p_tif, '', new_root=p_out).parent / 'plots'\n",
    "    p_plot.mkdir(exist_ok=True)\n",
    "\n",
    "    if utl.fname(p_tif, 'ch1.tif', new_root=p_out).is_file() and not overwrite:\n",
    "        print(f'INFO output file exists, skipping registration for {p_tif.parent}')\n",
    "        continue\n",
    "    else:\n",
    "        print(f'INFO now registering {p_tif}')\n",
    "\n",
    "    # load and split\n",
    "    stack = utl.load_tiff(p_tif)\n",
    "    ch1, ch2 = utl.split_channels(stack, n_z=15, n_ch=2)\n",
    "    ch1 = utl.maxproj_z(ch1)\n",
    "    ch2 = utl.maxproj_z(ch2)\n",
    "\n",
    "    ch1 = utl.smooth_xy(ch1, xy_smth)\n",
    "    ch2 = utl.smooth_xy(ch2, xy_smth)\n",
    "\n",
    "    # register\n",
    "    tmats = utl.get_tmats(ch2, reg)\n",
    "    ch1_a = utl.align(ch1, tmats, reg)\n",
    "    ch2_a = utl.align(ch2, tmats, reg)\n",
    "\n",
    "    # mean image\n",
    "    ch1_am = np.mean(ch1_a, axis=0)\n",
    "    ch2_am = np.mean(ch2_a, axis=0)\n",
    "\n",
    "    # save to disk\n",
    "    utl.write_tif(utl.fname(p_tif, 'ch1.tif', new_root=p_out), ch1_a.astype('int16'))\n",
    "    utl.write_tif(utl.fname(p_tif, 'ch2.tif', new_root=p_out), ch2_a.astype('int16'))\n",
    "\n",
    "    utl.write_tif(utl.fname(p_tif, 'ch1reg.tif', new_root=p_out), ch1_a.astype('int16'))\n",
    "    utl.write_tif(utl.fname(p_tif, 'ch2reg.tif', new_root=p_out), ch2_a.astype('int16'))\n",
    "\n",
    "    utl.save_img(p_plot / 'ch1mean.bmp', ch1_am)\n",
    "    utl.save_img(p_plot / 'ch2mean.bmp', ch2_am)\n",
    "\n",
    "    utl.save_dual_movie(p_plot / 'ch1ch2.mp4', ch1, ch2)\n",
    "    utl.save_dual_movie(p_plot / 'ch1reg.mp4', ch1, ch1_a)\n",
    "    utl.save_dual_movie(p_plot / 'ch2reg.mp4', ch2, ch2_a)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ROI extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_tif in p_tifs:\n",
    "    print()\n",
    "\n",
    "    # define plot folder\n",
    "    p_plot = utl.fname(p_tif, '', new_root=p_out).parent / 'plots'\n",
    "    p_plot.mkdir(exist_ok=True)\n",
    "    \n",
    "    # check if ROI traces have already been extracted\n",
    "    p_roi = utl.fname(p_tif, 'roi_traces.pickle', new_root=p_out)\n",
    "    if p_roi.is_file() and not overwrite:\n",
    "        print(f'INFO output files exists, skipping ROI extraction for {p_tif.parent}')\n",
    "        continue\n",
    "\n",
    "    # load Roi.zip\n",
    "    p_zip = utl.get_roi_zip_file(p_tif)\n",
    "    if not p_zip:\n",
    "        print(f'WARNING Skipping {p_tif.parent}')\n",
    "        continue\n",
    "    else:\n",
    "        print(f'INFO loading ROIs from {p_zip}')\n",
    "\n",
    "    # load aligned data\n",
    "    ch1_a = utl.load_tiff(utl.fname(p_tif, 'ch1reg.tif', new_root=p_out))\n",
    "    ch2_a = utl.load_tiff(utl.fname(p_tif, 'ch2reg.tif', new_root=p_out))\n",
    "\n",
    "    # load ROIs\n",
    "    img = np.mean(ch1_a, axis=0)\n",
    "    rois = utl.read_imagej_rois(p_zip, img)\n",
    "    img_rois = utl.draw_rois(img, rois)\n",
    "    utl.save_img(p_plot / 'ch1mean_rois.bmp', img_rois)\n",
    "\n",
    "    # extract traces\n",
    "    d_roi = dict() # collect traces extracted with different methods here\n",
    "\n",
    "    ca1 = utl.get_mean_trace(rois, ch1_a)\n",
    "    ca2 = utl.get_mean_trace(rois, ch2_a)\n",
    "    d_roi['ch1raw'] = ca1\n",
    "    d_roi['ch2raw'] = ca2\n",
    "\n",
    "    # channel 1 to 2 ratio\n",
    "    r12 = ca1 / ca2\n",
    "    d_roi['r12'] = r12\n",
    "\n",
    "    # subtract baseline\n",
    "    dca1 = utl.subtract_baseline(ca1, percentile=perc, size=winsize)\n",
    "    dca2 = utl.subtract_baseline(ca2, percentile=perc, size=winsize)\n",
    "    dr12 = utl.subtract_baseline(r12, percentile=perc, size=winsize)\n",
    "    d_roi['dch1'] = dca1\n",
    "    d_roi['dch2'] = dca2\n",
    "    d_roi['dr12'] = dr12\n",
    "\n",
    "    # subract background ROI\n",
    "    ca1 = utl.subtract_background(ca1)\n",
    "    ca2 = utl.subtract_background(ca2)\n",
    "    d_roi['ch1'] = ca1\n",
    "    d_roi['ch2'] = ca2  \n",
    "\n",
    "    # save to disk\n",
    "    with open(p_roi, 'wb') as f:\n",
    "        pickle.dump(d_roi, f)\n",
    "    print(f'INFO saving ROI traces to {p_roi}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Combine imaging and behavior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_tif in p_tifs:\n",
    "    print()\n",
    "\n",
    "    # define plot folder\n",
    "    p_plot = utl.fname(p_tif, '', new_root=p_out).parent / 'plots'\n",
    "    p_plot.mkdir(exist_ok=True)\n",
    "\n",
    "    # load ROI traces\n",
    "    p_roi = utl.fname(p_tif, 'roi_traces.pickle', new_root=p_out)\n",
    "    if not p_roi.is_file():\n",
    "        print(f'WARNING file with ROI traces not found, skipping {p_tif.parent}')\n",
    "    else:\n",
    "        with open(p_roi, 'rb') as f:\n",
    "            d_roi = pickle.load(f)\n",
    "\n",
    "    # load behavior data and ball velocities\n",
    "    p_mats = utl.get_matlab_files(p_tif)\n",
    "    if not p_mats:\n",
    "        print(f'WARNING skipping {p_tif.parent}')\n",
    "    else:\n",
    "        p_ball, p_beh =  p_mats\n",
    "        \n",
    "    ball = utl.load_ball(p_ball)\n",
    "    beh = utl.load_behavior(p_beh)\n",
    "    \n",
    "    for method, traces in d_roi.items():\n",
    "\n",
    "        # check if already been processed\n",
    "        p_df = utl.fname(p_tif, f'data_{method}.parquet', new_root=p_out)\n",
    "        if p_df.is_file() and not overwrite:\n",
    "            print(f'INFO output files exists, skipping data merging for method {method} in {p_tif.parent}')\n",
    "            continue\n",
    "\n",
    "        # match sample rates\n",
    "        df = utl.upsample_to_behavior(traces, beh, ball, f_ca, f_ball, f_beh)\n",
    "        # zscore ROIs\n",
    "        df = utl.zscore_cols(df, col_start='roi_')\n",
    "        # convolute ball velocities and behavior with Ca kernel\n",
    "        df = utl.convolute_ca_kernel(df, f=f_beh)\n",
    "        # zscore ball velocities\n",
    "        df = utl.zscore_cols(df, col_start='conv_ball_')\n",
    "\n",
    "        # add additional data based on file and folder names\n",
    "        pt = p_tif.parts\n",
    "        cond, fly, trial = pt[-5], pt[-4], pt[-2]\n",
    "        df.loc[:, 'cond'] = cond # e.g. fed/starved\n",
    "        df.loc[:, 'fly'] = fly # fly number\n",
    "        df.loc[:, 'trial'] = trial # trial number\n",
    "        print(f'INFO parsing folder names: fly {fly} | trial {trial} | condition {cond}')\n",
    "\n",
    "        # plots for quality control\n",
    "        utl.plot_data(df, f_beh, path=p_plot / f'data_{method}.png')\n",
    "        # pearson r heatmap\n",
    "        utl.plot_corr_heatmap(df, beh='behi', path=p_plot / f'heatmap_{method}.png')\n",
    "        # # ccf\n",
    "        # utl.plot_ccf(df, f=f_beh, pool_fly=True, path=p_plot / f'ccf_{method}.png')\n",
    "\n",
    "        # save to disk\n",
    "        print(f'INFO writing merged data to {p_df}')\n",
    "        df.to_parquet(p_df)\n",
    "\n",
    "        # optional (will be big files): save also as CSV\n",
    "        # df.to_csv(p_df.with_suffix('.csv')) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: merge all trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all trials and flies\n",
    "\n",
    "# get methods\n",
    "all_pars = []\n",
    "for p in p_tifs:\n",
    "    l = [ *utl.fname(p, '', new_root=p_out).parent.glob('*data_*.parquet') ]\n",
    "    all_pars.extend(l)\n",
    "methods = { p.stem.split('_')[-1] for p in all_pars }\n",
    "\n",
    "for method in methods:\n",
    "    # list of all *_data_{method}.parquet files\n",
    "    p_pars = [ utl.fname(p, f'data_{method}.parquet', new_root=p_out) for p in p_tifs ]\n",
    "\n",
    "    l = []\n",
    "    for p_par in p_pars:\n",
    "        if not p_par.is_file():\n",
    "            print(f'WARNING skipping {p_par.parent}')\n",
    "            continue\n",
    "        else:\n",
    "            print(f'INFO loading file {p_par}')\n",
    "            df = pd.read_parquet(p_par)\n",
    "            l.append(df)\n",
    "\n",
    "    if l:\n",
    "        # combine dataframes and save\n",
    "        df = pd.concat(l, ignore_index=True)\n",
    "        p_df = p_out / f'all_data_{method}.parquet'\n",
    "        df.to_parquet(p_df)\n",
    "\n",
    "        print(f'INFO contents of {p_df}')\n",
    "        for f, d in df.groupby('fly'):\n",
    "            print(f'     {f}', end=': ')\n",
    "            for t, _ in d.groupby('trial'):\n",
    "                print(f'{t}', end=' ')\n",
    "            print()\n",
    "    else:\n",
    "        # this check should not be necessary (?)\n",
    "        print(f'WARNING no data files found, skipping {method}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all trials and flies\n",
    "for p_df in p_out.glob('all_data_*.parquet'):\n",
    "    method = p_df.stem.split('_')[-1]\n",
    "\n",
    "    # create plot folder\n",
    "    p_plot = p_out / method\n",
    "    p_plot.mkdir(exist_ok=True)\n",
    "\n",
    "    # read data from disk\n",
    "    df = pd.read_parquet(p_df)\n",
    "    df = df.fillna(0) # TODO workaround because of missing behavior\n",
    "\n",
    "    # optional: remove ROI 7, 8, 9\n",
    "    df = df.drop(columns=['z_roi_7', 'z_roi_8', 'z_roi_9'])\n",
    "\n",
    "    # pearson correlation heatmap (selection of columns: see utl.calculate_pearson)\n",
    "    utl.plot_corr_heatmap(df, beh='behi', path=p_plot / 'heatmap.svg')\n",
    "\n",
    "    # pearson heatmaps around behavior events\n",
    "    dt = 5 # time in s to keep before and after behavior event\n",
    "\n",
    "    # loop through all behavior\n",
    "    cols = [ c for c in df.columns if c.startswith('beh_') ]\n",
    "    for col in cols:\n",
    "\n",
    "        # select df around behavoir\n",
    "        d = utl.select_event(df, col, f_beh, dt)\n",
    "\n",
    "        # generate plot\n",
    "        utl.plot_corr_heatmap(d, beh='behi', path=p_plot / f'heatmap_{col}.svg')\n",
    "\n",
    "    # # cross-correlation functions behavior/ball and ROIs (averaged)\n",
    "    # utl.plot_ccf(df, f=f_beh, pool_fly=True,  path=p_plot / 'ccf.svg')\n",
    "\n",
    "    # # same (not averaged)\n",
    "    # utl.plot_ccf(df, f=f_beh, pool_fly=False, path=p_plot / 'ccf_indv.svg')\n",
    "\n",
    "    # plot aligned data\n",
    "    dt = 5 # time in s before and after behavior event\n",
    "    s = 0.25 # smoothing window for velocity [in s] \n",
    "\n",
    "    # smooth velocity\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, ['ball_x', 'ball_y', 'ball_z']] = gaussian_filter(df_.loc[:, ['ball_x', 'ball_y', 'ball_z']].values, (s * f_beh, 0))\n",
    "\n",
    "    # cycle through all behavoirs\n",
    "    cols = [c for c in df_.columns if c.startswith('beh_')]\n",
    "    for col in cols:\n",
    "\n",
    "        # align to behavior\n",
    "        df_al = utl.align2events(df_, col, f_beh, dt)\n",
    "\n",
    "        utl.plot_aligned(df_al, path=p_plot / f'aligned_to_{col}.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spatial correlation maps (long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tif file for session of interest\n",
    "for p_tif in p_tifs:\n",
    "\n",
    "    # define and create output folder\n",
    "    p_plot = utl.fname(p_tif, '', new_root=p_out).parent / 'corrmaps'\n",
    "    p_plot.mkdir(exist_ok=True)\n",
    "    print(p_plot)\n",
    "\n",
    "    # load registered tif files\n",
    "    ch1 = utl.load_tiff(utl.fname(p_tif, 'ch1reg.tif', new_root=p_out))\n",
    "    ch2 = utl.load_tiff(utl.fname(p_tif, 'ch2reg.tif', new_root=p_out))\n",
    "\n",
    "    # load preprocessed behavior data\n",
    "    df = pd.read_parquet(utl.fname(p_tif, 'data_ch1.parquet', new_root=p_out)) #either of the data files work\n",
    "\n",
    "    # loop through all conv behi and conv ball columns\n",
    "    cols = [ c for c in df.columns if c.startswith('conv_behi') or c.startswith('conv_ball') ]\n",
    "    for col in cols:\n",
    "        utl.plot_corrmap(ch1, ch2, df, col, f_ca=f_ca, f_beh=f_beh, path=p_plot / f'{col}_1xy.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to folder\n",
    "ps = [ *parent_dir.glob('female13/trials_to_register/*/*0000?.tif') ]\n",
    "\n",
    "p_plot = p_out / 'pooled_corrmap'\n",
    "p_plot.mkdir(exist_ok=True)\n",
    "\n",
    "# combine tifs\n",
    "l1, l2 = [], []\n",
    "for p_tif in ps:\n",
    "    ch1 = utl.load_tiff(utl.fname(p_tif, 'ch1.tif', new_root=p_out))\n",
    "    ch2 = utl.load_tiff(utl.fname(p_tif, 'ch2.tif', new_root=p_out))\n",
    "    l1.append(ch1)\n",
    "    l2.append(ch2)\n",
    "\n",
    "ch1 = np.concatenate(l1, axis=0)\n",
    "ch2 = np.concatenate(l2, axis=0)\n",
    "\n",
    "# register\n",
    "tmats = utl.get_tmats(ch2, reg)\n",
    "ch1_a = utl.align(ch1, tmats, reg)\n",
    "ch2_a = utl.align(ch2, tmats, reg)\n",
    "\n",
    "# mean image\n",
    "ch1_am = np.mean(ch1_a, axis=0)\n",
    "ch2_am = np.mean(ch2_a, axis=0)\n",
    "\n",
    "# save to disk\n",
    "utl.write_tif(p_plot / 'ch1.tif', ch1_a.astype('int16'))\n",
    "utl.write_tif(p_plot / 'ch2.tif', ch2_a.astype('int16'))\n",
    "\n",
    "utl.write_tif(p_plot / 'ch1reg.tif', ch1_a.astype('int16'))\n",
    "utl.write_tif(p_plot / 'ch2reg.tif', ch2_a.astype('int16'))\n",
    "\n",
    "utl.save_img(p_plot / 'ch1mean.bmp', ch1_am)\n",
    "utl.save_img(p_plot / 'ch2mean.bmp', ch2_am)\n",
    "\n",
    "utl.save_dual_movie(p_plot / 'ch1ch2.mp4', ch1, ch2)\n",
    "utl.save_dual_movie(p_plot / 'ch1reg.mp4', ch1, ch1_a)\n",
    "utl.save_dual_movie(p_plot / 'ch2reg.mp4', ch2, ch2_a)\n",
    "\n",
    "# load preprocessed behavior data\n",
    "l = []\n",
    "for p_tif in ps:\n",
    "    df = pd.read_parquet(utl.fname(p_tif, 'data_ch1.parquet', new_root=p_out)) #either of the data works\n",
    "    l.append(df)\n",
    "df = pd.concat(l, ignore_index=True)\n",
    "\n",
    "# loop through all conv behi and conv ball columns\n",
    "cols = [ c for c in df.columns if c.startswith('conv_behi') or c.startswith('conv_ball') ]\n",
    "for col in cols:\n",
    "    utl.plot_corrmap(ch1, ch2, df, col, f_ca=f_ca, f_beh=f_beh, path=p_plot / f'{col}_1xy.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imaging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
